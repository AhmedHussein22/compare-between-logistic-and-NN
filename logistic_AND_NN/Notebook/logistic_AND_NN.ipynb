{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic_AND_NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2e39IYpgYGZ"
      },
      "source": [
        "* logistic regression *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ4xLQYpYiH-"
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "np.random.seed(1)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VxixUfcYxeQ"
      },
      "source": [
        "def load_dataset():\n",
        "    train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n",
        "    train_set_x_original = np.array(train_dataset[\"train_set_x\"][:])  # your train set features\n",
        "    train_set_y_original = np.array(train_dataset[\"train_set_y\"][:])  # your train set labels\n",
        "\n",
        "    test_dataset = h5py.File('test_catvnoncat.h5', \"r\")\n",
        "    test_set_x_original = np.array(test_dataset[\"test_set_x\"][:])  # your test set features\n",
        "    test_set_y_original = np.array(test_dataset[\"test_set_y\"][:])  # your test set labels\n",
        "\n",
        "    clause = np.array(test_dataset[\"list_classes\"][:])  # the list of classes\n",
        "\n",
        "    return train_set_x_original, train_set_y_original, test_set_x_original, test_set_y_original, clause"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6o8YEWyY-tL"
      },
      "source": [
        "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes = load_dataset()\n",
        "\n",
        "# print dataset shapes(dimensions)\n",
        "print(\"train set x dim= \" + str(train_set_x_orig.shape))\n",
        "print(\"train set y dim= \" + str(train_set_y_orig.shape))\n",
        "print(\"test set x dim= \" + str(test_set_x_orig.shape))\n",
        "print(\"test set y dim= \" + str(test_set_y_orig.shape))\n",
        "\n",
        "# reshape dataset\n",
        "# train data\n",
        "train_set_x = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
        "train_set_y = train_set_y_orig.reshape(1, train_set_y_orig.shape[0])\n",
        "\n",
        "# test data\n",
        "test_set_x = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
        "test_set_y = test_set_y_orig.reshape(1, test_set_y_orig.shape[0])\n",
        "\n",
        "# print dataset shapes(dimensions)\n",
        "print(\"train set x dim= \" + str(train_set_x.shape))\n",
        "print(\"train set y dim= \" + str(train_set_y.shape))\n",
        "print(\"test set x dim= \" + str(test_set_x.shape))\n",
        "print(\"test set y dim= \" + str(test_set_y.shape))\n",
        "\n",
        "# normalize data\n",
        "train_set_x = train_set_x / 255\n",
        "test_set_x = test_set_x / 255\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM0XgYAxZNOE"
      },
      "source": [
        "def sigmoid(input):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of input\n",
        "\n",
        "    Arguments:\n",
        "    input -- A scalar or numpy array of any size.\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(input)\n",
        "    \"\"\"\n",
        "\n",
        "    s = 1 / (1 + np.exp(-input))\n",
        "\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4ffIeUcZSZA"
      },
      "source": [
        "def initialize_with_zeros(dim):\n",
        "    \"\"\"\n",
        "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
        "\n",
        "    Argument:\n",
        "    dim -- number of parameters\n",
        "\n",
        "    Returns:\n",
        "    w -- initialized vector of shape (dim, 1)\n",
        "    b -- initialized scalar (corresponds to the bias)\n",
        "    \"\"\"\n",
        "\n",
        "    w = np.zeros((dim, 1))\n",
        "    b = 0.0\n",
        "\n",
        "    return w, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHxOyTkYZcPa"
      },
      "source": [
        "def propagate(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function and its gradient\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
        "\n",
        "    Return:\n",
        "    cost -- negative log-likelihood cost for logistic regression\n",
        "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
        "    db -- gradient of the loss with respect to b, thus same shape as b\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[1]\n",
        "\n",
        "    # FORWARD PROPAGATION\n",
        "\n",
        "    A = sigmoid(np.dot(w.T, X) + b)  # compute activation\n",
        "    cost = - 1 / m * np.sum((Y * np.log(A) + (1 - Y) * np.log(1 - A)))  # compute cost\n",
        "\n",
        "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
        "\n",
        "    dw = 1 / m * np.dot(X, (A - Y).T)\n",
        "    db = 1 / m * np.sum(A - Y)\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "    return grads, cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP-Ch4KhZhcT"
      },
      "source": [
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "\n",
        "    Returns:\n",
        "    params -- dictionary containing the weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "\n",
        "    Notes:\n",
        "    there are two steps to iterate through them:\n",
        "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
        "        2) Update the parameters using gradient descent rule for w and b.\n",
        "    \"\"\"\n",
        "\n",
        "    costs = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "\n",
        "        grads, cost = propagate(w, b, X, Y)\n",
        "\n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "\n",
        "        # updates\n",
        "        w = w - learning_rate * dw\n",
        "        b = b - learning_rate * db\n",
        "\n",
        "        # Record the costs\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "        # Print the cost every 100 training examples\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
        "\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "    return params, grads, costs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9Y7bljBZmv4"
      },
      "source": [
        "def predictlogisticregression(w, b, X):\n",
        "    \"\"\"\n",
        "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px * 3, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1, m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "\n",
        "    ### make prediction\n",
        "    A = sigmoid(np.dot(w.T, X) + b)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    for i in range(A.shape[1]):\n",
        "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
        "        if A[0, i] >= 0.5:\n",
        "            Y_prediction[0, i] = 1\n",
        "        else:\n",
        "            Y_prediction[0, i] = 0\n",
        "\n",
        "    return Y_prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGqOwJY_Zp74"
      },
      "source": [
        "*** logistic regression model ***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5rwZVUxZ5KM"
      },
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function you've implemented previously\n",
        "\n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "\n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize\n",
        "    w, b = initialize_with_zeros(X_train.shape[0])  # where to get dimension?\n",
        "\n",
        "    # Gradient descent\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "\n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "\n",
        "    # Predict test/train set examples\n",
        "    Y_prediction_test = predictlogisticregression(w, b, X_test)\n",
        "    Y_prediction_train = predictlogisticregression(w, b, X_train)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Print train/test accuracy\n",
        "    print(\"=================== accuracy of logistic regression ================\")\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test,\n",
        "         \"Y_prediction_train\": Y_prediction_train,\n",
        "         \"w\": w,\n",
        "         \"b\": b,\n",
        "         \"learning_rate\": learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "\n",
        "    return d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4GOFiIXggWG"
      },
      "source": [
        "****************************** NN******************************* *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH4MDeZ3goV5"
      },
      "source": [
        "# GRADED FUNCTION: layer_sizes\n",
        "\n",
        "def layer_sizes(X, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- input dataset of shape (input size, number of examples)\n",
        "    Y -- labels of shape (output size, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    n_x -- the size of the input layer\n",
        "    n_h -- the size of the hidden layer\n",
        "    n_y -- the size of the output layer\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (≈ 3 lines of code)\n",
        "    n_x = X.shape[0]  # size of input layer\n",
        "    n_h = 4\n",
        "    n_y = Y.shape[0]  # size of output layer\n",
        "    ### END CODE HERE ###\n",
        "    return [n_x, n_h, n_y]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnZbSM3wgwi3"
      },
      "source": [
        "# GRADED FUNCTION: initialize_parameters\n",
        "\n",
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    n_x -- size of the input layer\n",
        "    n_h -- size of the hidden layer\n",
        "    n_y -- size of the output layer\n",
        "\n",
        "    Returns:\n",
        "    params -- python dictionary containing your parameters:\n",
        "                    W1 -- weight matrix of shape (n_h, n_x)\n",
        "                    b1 -- bias vector of shape (n_h, 1)\n",
        "                    W2 -- weight matrix of shape (n_y, n_h)\n",
        "                    b2 -- bias vector of shape (n_y, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(2)  # we set up a seed so that your output matches ours although the initialization is random.\n",
        "\n",
        "    ### START CODE HERE ### (≈ 4 lines of code)\n",
        "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
        "    b1 = np.zeros((n_h, 1))\n",
        "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
        "    b2 = np.zeros((n_y, 1))\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert (W1.shape == (n_h, n_x))\n",
        "    assert (b1.shape == (n_h, 1))\n",
        "    assert (W2.shape == (n_y, n_h))\n",
        "    assert (b2.shape == (n_y, 1))\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo_v1h_5g0xT"
      },
      "source": [
        "# GRADED FUNCTION: forward_propagation\n",
        "\n",
        "def forward_propagation(X, allParameters):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    X -- input data of size (n_x, m)\n",
        "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
        "\n",
        "    Returns:\n",
        "    A2 -- The sigmoid output of the second activation\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    ### START CODE HERE ### (≈ 4 lines of code)\n",
        "    W1 = allParameters[\"W1\"]\n",
        "    b1 = allParameters[\"b1\"]\n",
        "    W2 = allParameters[\"W2\"]\n",
        "    b2 = allParameters[\"b2\"]\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
        "    ### START CODE HERE ### (≈ 4 lines of code)\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = np.tanh(Z1)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert (A2.shape == (1, X.shape[1]))\n",
        "\n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "\n",
        "    return A2, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmOuokfhg6Bw"
      },
      "source": [
        "# GRADED FUNCTION: compute_cost\n",
        "\n",
        "def compute_cost(A2, Y, parameters):\n",
        "    \"\"\"\n",
        "    Computes the cross-entropy cost given in equation (13)\n",
        "\n",
        "    Arguments:\n",
        "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
        "    [Note that the parameters argument is not used in this function,\n",
        "    but the auto-grader currently expects this parameter.\n",
        "    Future version of this notebook will fix both the notebook\n",
        "    and the auto-grader so that `parameters` is not needed.\n",
        "    For now, please include `parameters` in the function signature,\n",
        "    and also when invoking this function.]\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost given equation (13)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    m = Y.shape[1]  # number of example\n",
        "\n",
        "    # Compute the cross-entropy cost\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y)\n",
        "    # logprobs = np.multiply(Y,np.log(A2))+np.multiply(1-Y,np.log(1-A2))\n",
        "    cost = -1 / m * np.sum(logprobs)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect.\n",
        "    # E.g., turns [[17]] into 17\n",
        "    assert (isinstance(cost, float))\n",
        "\n",
        "    return cost"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1znRCHYzg_dX"
      },
      "source": [
        "# GRADED FUNCTION: backward_propagation\n",
        "\n",
        "def backward_propagation(parameters, cache, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation using the instructions above.\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing our parameters\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
        "    X -- input data of shape (2, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    grads -- python dictionary containing your gradients with respect to different parameters\n",
        "    \"\"\"\n",
        "    m = X.shape[1]\n",
        "\n",
        "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    W1 = parameters[\"W1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    A1 = cache[\"A1\"]\n",
        "    A2 = cache[\"A2\"]\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Backward propagation: calculate dW1, db1, dW2, db2.\n",
        "    ### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)\n",
        "    dZ2 = cache[\"A2\"] - Y\n",
        "    dW2 = 1 / m * np.dot(dZ2, cache[\"A1\"].T)\n",
        "    db2 = 1 / m * np.sum(dZ2, axis=1, keepdims=True)\n",
        "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
        "    dW1 = 1 / m * np.dot(dZ1, X.T)\n",
        "    db1 = 1 / m * np.sum(dZ1, axis=1, keepdims=True)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "\n",
        "    return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzP3UqKDhDZh"
      },
      "source": [
        "# GRADED FUNCTION: update_parameters\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate=1.2):\n",
        "    \"\"\"\n",
        "    Updates parameters using the gradient descent update rule given above\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters\n",
        "    grads -- python dictionary containing your gradients\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters\n",
        "    \"\"\"\n",
        "    # Retrieve each parameter from the dictionary \"parameters\"\n",
        "    ### START CODE HERE ### (≈ 4 lines of code)\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Retrieve each gradient from the dictionary \"grads\"\n",
        "    ### START CODE HERE ### (≈ 4 lines of code)\n",
        "    dW1 = grads[\"dW1\"]\n",
        "    db1 = grads[\"db1\"]\n",
        "    dW2 = grads[\"dW2\"]\n",
        "    db2 = grads[\"db2\"]\n",
        "    ## END CODE HERE ###\n",
        "\n",
        "    # Update rule for each parameter\n",
        "    ### START CODE HERE ### (≈ 4 lines of code)\n",
        "    W1 = W1 - learning_rate * dW1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    W2 = W2 - learning_rate * dW2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNNx4D0QhJEA"
      },
      "source": [
        "# ********************** nn_model *******************************\n",
        "\n",
        "def nn_model(X, Y, n_h, num_iterations=10000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    X -- dataset of shape (2, number of examples)\n",
        "    Y -- labels of shape (1, number of examples)\n",
        "    n_h -- size of the hidden layer\n",
        "    num_iterations -- Number of iterations in gradient descent loop\n",
        "    print_cost -- if True, print the cost every 1000 iterations\n",
        "\n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(3)\n",
        "    n_x = layer_sizes(X, Y)[0]\n",
        "    n_y = layer_sizes(X, Y)[2]\n",
        "\n",
        "    # Initialize parameters\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Loop (gradient descent)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        ### START CODE HERE ### (≈ 4 lines of code)\n",
        "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
        "        A2, cache = forward_propagation(X, parameters)\n",
        "\n",
        "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
        "        cost = compute_cost(A2, Y, parameters)\n",
        "\n",
        "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
        "        grads = backward_propagation(parameters, cache, X, Y)\n",
        "\n",
        "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
        "        parameters = update_parameters(parameters, grads)\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Print the cost every 1000 iterations\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G63TN3bThOUM"
      },
      "source": [
        "# GRADED FUNCTION: predict\n",
        "\n",
        "def predict(parameters, X):\n",
        "    \"\"\"\n",
        "    Using the learned parameters, predicts a class for each example in X\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters\n",
        "    X -- input data of size (n_x, m)\n",
        "\n",
        "    Returns\n",
        "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    A2, cache = forward_propagation(X, parameters)\n",
        "    # predictions = (A2 > 0.5)\n",
        "    predictions = (A2 > 0.5)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g224jegChTX8"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    print(\"=================== logistic regression ================\")\n",
        "    model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005,\n",
        "          print_cost=True)\n",
        "    print(\"\\n=================== NN ================\")\n",
        "    parameters = nn_model(train_set_x, train_set_y, n_h=4, num_iterations=10000, print_cost=True)\n",
        "    print(\"===================Accuracy of  NN  ================\")\n",
        "    predictions = predict(parameters, train_set_x)\n",
        "    print('Accuracy: %d' % float(\n",
        "        (np.dot(train_set_y, predictions.T) + np.dot(1 - train_set_y, 1 - predictions.T)) / float(train_set_y.size) * 100) + '%')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}